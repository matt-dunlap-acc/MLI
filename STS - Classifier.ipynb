{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1c3d52da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "import csv #to read in and split up all the content from the dataset input file\n",
    "import numpy as np #import numpy using np as an alias\n",
    "import pandas as pd #import pandas using pd as an alias\n",
    "import matplotlib.pyplot as plt #import matplotlib.pyplot using plt as an alias\n",
    "import seaborn as sb #import seaborn using sns as an alias\n",
    "import sys\n",
    "import codecs\n",
    "# Scikit-Learn â‰¥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder #import the encoder classes\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12ec7b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineDF(dfExtension, df):\n",
    "    if dfExtension == 0:\n",
    "        result = pd.read_csv('D:\\STS Master Deck - OneHot Encoded\\DF'+ str(dfExtension) + '.csv', delimiter=',', index_col=False)\n",
    "    else:\n",
    "        df1 = pd.read_csv('D:\\STS Master Deck - OneHot Encoded\\DF'+ str(dfExtension) + '.csv', delimiter=',', index_col=False)\n",
    "        frames = [df, df1]\n",
    "        result = pd.concat(frames)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2930f926",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = 0\n",
    "for dfExtension in range(0, 180000, 10000):\n",
    "    df = combineDF(dfExtension, df)\n",
    "\n",
    "df.to_csv('D:\\STS Master Deck - OneHot Encoded\\MasterDf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3e486826",
   "metadata": {},
   "outputs": [],
   "source": [
    "playsArray = df.to_numpy(copy=True)\n",
    "trainSet, testSet = train_test_split(playsArray, test_size = .2, random_state = 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4e273439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#array[row][column]\n",
    "trainData, trainLabels, testData, testLabels = np.delete(trainSet, 2, 1), trainSet[:,2].astype(int), np.delete(testSet, 2, 1), testSet[:,2].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b0167aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Failed Runs Percent: 0.596815669428653\n"
     ]
    }
   ],
   "source": [
    "totalLabels = len(trainLabels)\n",
    "totalFailedRuns = 0\n",
    "for label in trainLabels:\n",
    "    if (label == 0):\n",
    "        totalFailedRuns += 1\n",
    "print(f\"Total Failed Runs Percent: {totalFailedRuns / totalLabels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b667de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsList = [(\"forestClassifier\", RandomForestClassifier(random_state=42)),\n",
    "                (\"extraTreesClassifier\", ExtraTreesClassifier(random_state=42)),\n",
    "                (\"linearSVC\", LinearSVC(random_state=42))]\n",
    "testList = [(\"linearSVC\", LinearSVC(random_state=42))]\n",
    "otherTwoModels = [(\"forestClassifier\", RandomForestClassifier(random_state=42)),\n",
    "                (\"extraTreesClassifier\", ExtraTreesClassifier(random_state=42))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cbdf1e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCrossValScores(modelsList, trainData, trainLabels):\n",
    "    output = \"\"\n",
    "    for key, model in modelsList:\n",
    "        accuracy = cross_val_score(model, trainData, trainLabels,\n",
    "                                 scoring=\"accuracy\", cv=3, )\n",
    "        output += (f\"Model: {key}\\nScore: {accuracy}\\nScore Mean with 3 Folds: {accuracy.mean()}\\n\\n\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "65f3e3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dunla\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\dunla\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\dunla\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: linearSVC\n",
      "Score: [0.61599622 0.41593757 0.67111019]\n",
      "Score Mean with 3 Folds: 0.5676813273802247\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(getCrossValScores(testList, trainData, trainLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "443ee218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: forestClassifier\n",
      "Score: [0.69411521 0.69558406 0.69446087]\n",
      "Score Mean with 3 Folds: 0.6947200469455647\n",
      "\n",
      "Model: extraTreesClassifier\n",
      "Score: [0.68864718 0.69197801 0.6902341 ]\n",
      "Score Mean with 3 Folds: 0.6902864285569995\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(getCrossValScores(otherTwoModels, trainData, trainLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8642fec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forest and Extra-Trees significantly outperformed SVC thus the latter is not\n",
    "#included in the ensemble.\n",
    "modelsForEnsemble = [(\"forestClassifier\", RandomForestClassifier(random_state=42)),\n",
    "                (\"extraTreesClassifier\", ExtraTreesClassifier(random_state=42))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2e0e50dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEnsembleAccuracy(classifierModelsList, trainData, trainLabels, testData, testLabels):\n",
    "    result = \"\"\n",
    "    \n",
    "    voting_clf = VotingClassifier(classifierModelsList)\n",
    "    voting_clf.fit(trainData, trainLabels)\n",
    "    \n",
    "    #set to soft voting and display soft voting accuracy\n",
    "    voting_clf.voting = \"soft\"\n",
    "    result += (f\"Soft Voting Accuracy: {voting_clf.score(testData, testLabels)}\\n\")\n",
    "    \n",
    "    #set to hard voting and display hard voting accuracy\n",
    "    voting_clf.voting = \"hard\"\n",
    "    result += (f\"Hard Voting Accuracy: {voting_clf.score(testData, testLabels)}\\n\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b33d64ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft Voting Accuracy: 0.699743842364532\n",
      "Hard Voting Accuracy: 0.6893004926108375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(getEnsembleAccuracy(modelsForEnsemble, trainData, trainLabels, testData, testLabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aacd9d",
   "metadata": {},
   "source": [
    "## Evaluating The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e360e271",
   "metadata": {},
   "source": [
    "### Check percentage of labels with 0 and compare against model's accuracy.\n",
    "#### - Percentage of labels with 0 in training set: 59% \n",
    "#### - Model's accuracy: 69%\n",
    "#### - Conclusion: the accuracy of the model is not determined by the percent occurance of the majority label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "19b91aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Failed Runs Percent: 0.596815669428653\n"
     ]
    }
   ],
   "source": [
    "#Check percentage of labels with 0 and compare against model's accuracy\n",
    "totalLabels = len(trainLabels)\n",
    "totalFailedRuns = 0\n",
    "for label in trainLabels:\n",
    "    if (label == 0):\n",
    "        totalFailedRuns += 1\n",
    "print(f\"Total Failed Runs Percent: {totalFailedRuns / totalLabels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1d3ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
